import geopandas as gpd
import contextily as ctx
import matplotlib.pyplot as plt
import folium
import pandas as pd
import urllib3
import requests
import osmnx as ox
import networkx as nx
import numpy as np
from tqdm.auto import tqdm 

from pysal.lib import weights
from pysal.lib import cg as geometry

##########################################
##### General variables ##################
##########################################

categories_tags = {
        'restaurant':["restaurant", "cafe","bar","ice_cream","fast_food","pub","food_court","biergarten"],
        'culture and art':["library", "toy_library", "music_school","arts_centre",
        "cinema", "conference_centre", "events_venue", "planetarium", "public_bookcase",
        "studio", "theatre"],
        'education':["college", "driving_school", "kindergarten", "language_school", "training", "school", "university"]
    }


"""
We are building several functions on top of OSMnx library which itself build
to interact with Overpass API (a copy of OMS data without the same API limitations)
"""

##########################################
##### OSMNX functions specific implementation ########
##########################################

def get_place_POI_tags(place : str,
    tags = {"amenity":["restaurant", "cafe","bar","ice_cream","fast_food","pub","food_court","biergarten"]},
    city : str = "Paris, Ile-de-France, France", consolidate = True,
    network_type = 'walk') : 
    """
    Function to get any city's (Paris' by default) neighborhood's OMS POI.
    place : str, must be name sufficiently known
    city : str, format : "Name, Region, Country" (Example : Paris, Ile-de-France, France)
    tags : dict, keys and values are the same as in OMS. 
    For instance : https://wiki.openstreetmap.org/wiki/FR:%C3%89l%C3%A9ments_cartographiques#
    and : https://wiki.openstreetmap.org/wiki/FR:%C3%89l%C3%A9ments_cartographiques#Consommation
    consolidate : use the osmnx consolidate_intersections (tolerance = 15) to merge place with too complicated intersections like a roundabout 
    Returns the street network in a 1km walking distance as a networkx object 
    and the POI in the same area as a geodataframe

    Streets network and POI are projected to WGS-84
    """
    #récupération des données piétons
    place += ", "+city # complete addresse

    #get the network
    g_place = ox.graph_from_place(place, buffer_dist=1000, network_type=network_type, retain_all=True, truncate_by_edge=True)
    g_place = ox.project_graph(g_place, to_crs="WGS-84")
    if consolidate:
        g_proj = ox.project_graph(g_place)
        g_place = ox.consolidate_intersections(g_proj, rebuild_graph=True, tolerance=15, dead_ends=False)
    
    gdf_pois = ox.geometries_from_place(place, tags, buffer_dist=1000)
    #certains lieux (comme une ville) ont un polygone associée : 
    # on peut donc récupérer les POI sans indiquer de dist
    gdf_pois = ox.project_gdf(gdf= gdf_pois, to_crs="WGS-84")
    gdf_pois["center"]=gdf_pois.centroid
    #chaque ligne peut être soit un polygone (par exemple pour le champ de Mars), soit un point comme un restaurant : on calcul le centre pour avoir une référence unique
    return g_place, gdf_pois    #On récupère directement un networkx et un geodataframe


def get_place_POI_category(place: str, 
    categories : list,
    city : str = "Paris, Ile-de-France, France", consolidate = True,
    network_type = 'walk') :
    """
    Function to get any city's (Paris' by default) neighborhood's OMS POI.
    place : str, must be name sufficiently known
    city : str, format : "Name, Region, Country" (Example : Paris, Ile-de-France, France)
    categories : homemade categories on amenities tags. list of str between the following :
    Available categories : 'restaurant', 'culture and art', 'education' 
    list_restaurants = ["restaurant", "cafe","bar","ice_cream","fast_food","pub","food_court","biergarten"]
    list_culture = ["library", "toy_library", "music_school","arts_centre", "cinema", "conference_centre", "events_venue", "planetarium", "public_bookcase", "studio", "theatre"]
    list_education = ["college", "driving_school", "kindergarten", "language_school", "training", "school", "university"]
    See : https://wiki.openstreetmap.org/wiki/FR:%C3%89l%C3%A9ments_cartographiques#
    consolidate : use the osmnx consolidate_intersections (tolerance = 15) to merge place with too complicated intersections like a roundabout 
    Returns the street network in a 1km walking distance as a networkx object 
    and the POI in the same area as a geodataframe

    Streets network and POI are projected to WGS-84"""
    
    tags = []
    for cat in categories:
        tags += categories_tags[cat]
    tags = {'amenity':tags}
    print(tags)
    return get_place_POI_tags(place = place, tags = tags, city=city, consolidate=consolidate, network_type=network_type)

def get_polygon_POI_tags(
    polygon,
    tags = {"amenity":["restaurant", "cafe","bar","ice_cream","fast_food","pub","food_court","biergarten"]},
    consolidate = True,
    network_type = 'walk') : 
    """
    Function to get OMS POI within a polygon.
    polygon : shapely.geometry.MultiPolygon or shapely.geometry.Polygon
    tags : dict, keys and values are the same as in OMS. 
    For instance : https://wiki.openstreetmap.org/wiki/FR:%C3%89l%C3%A9ments_cartographiques#
    and : https://wiki.openstreetmap.org/wiki/FR:%C3%89l%C3%A9ments_cartographiques#Consommation
    consolidate : use the osmnx consolidate_intersections (tolerance = 15) to merge place with too complicated intersections like a roundabout 
    Returns the POI in the polygon as a geodataframe
    POI are projected to WGS-84
    """
    """# récupération des données piétons
    #get the network
    g_poly = ox.graph_from_polygon(polygon, network_type=network_type, retain_all=True, truncate_by_edge=True)
    g_poly = ox.project_graph(g_poly, to_crs="WGS-84")
    if consolidate:
        g_proj = ox.project_graph(g_poly)
        g_poly = ox.consolidate_intersections(g_proj, rebuild_graph=True, tolerance=15, dead_ends=False)"""
    
    gdf_pois = ox.geometries.geometries_from_polygon(polygon, tags)
    #certains lieux (comme une ville) ont un polygone associée : 
    # on peut donc récupérer les POI sans indiquer de dist
    if len(gdf_pois) > 0:
        gdf_pois = ox.project_gdf(gdf= gdf_pois, to_crs="WGS-84")
        #gdf_pois["center"]=gdf_pois.centroid
    #chaque ligne peut être soit un polygone (par exemple pour le champ de Mars), soit un point comme un restaurant : on calcul le centre pour avoir une référence unique
    return gdf_pois #return g_poly, gdf_pois    #On récupère directement un networkx et un geodataframe

def get_polygon_POI_category(polygon, 
    categories : list,
    consolidate = True,
    network_type = 'walk') :
    """
    Function to get OMS POI within a polygon.
    polygon : shapely.geometry.MultiPolygon or shapely.geometry.Polygon
    categories : homemade categories on amenities tags. list of str between the following :
    Available categories : 'restaurant', 'culture and art', 'education' 
    list_restaurants = ["restaurant", "cafe","bar","ice_cream","fast_food","pub","food_court","biergarten"]
    list_culture = ["library", "toy_library", "music_school","arts_centre", "cinema", "conference_centre", "events_venue", "planetarium", "public_bookcase", "studio", "theatre"]
    list_education = ["college", "driving_school", "kindergarten", "language_school", "training", "school", "university"]
    See : https://wiki.openstreetmap.org/wiki/FR:%C3%89l%C3%A9ments_cartographiques#
    consolidate : use the osmnx consolidate_intersections (tolerance = 15) to merge place with too complicated intersections like a roundabout 
    Returns the POI in the polygon as a geodataframe
    POI are projected to WGS-84"""
    
    tags = []
    for cat in categories:
        tags += categories_tags[cat]
    tags = {'amenity':tags}
    return get_polygon_POI_tags(polygon = polygon, tags = tags, consolidate=consolidate, network_type=network_type)

##########################################
##### Map function ########
########################################## 

def plot_POI_folium(place_latlong : np.array,
    gdf_poi: gpd.GeoDataFrame, tiles = "OpenStreetMap", zoom_start = 14) :
    """
    place_latlong : the central place for which to center the map on, tuple (lat, long)
    gdf_poi : GeoPandas.GeoDataFrame with POI
    It is the classical folium method : you create a list with geometry point values (here gdf_poi.center) and for each 
    coordinates, you add a marker. 
    Returns a Folium map
    """
    #on représente en lat, long (et donc y,x)!!!
    # y = lat
    # x = long
    map = folium.Map(location=place_latlong, tiles=tiles, zoom_start=zoom_start)

    for poi in gdf_poi.iterrows():
        # Place the markers with the popup labels and data
        x , y = (poi[1]['center'].x, poi[1]['center'].y)
        map.add_child(
            folium.Marker(
                location=[y,x],
                popup=
                    "Name: " + str(poi[1]['name']) + "<br>" #here to add name 
                    #+ "Leisure: " + str(poi[1]['leisure']) + "<br>" #type 
                    + "Amenity: " + str(poi[1]['amenity']) + "<br>" #type 
                    + "Coordinates: " + str(y)+', '+str(x)
                ,
            
                icon=folium.Icon(color="blue"),
            )
        )
    return map
##########################################
##### Route functions ########
##########################################

def route_between_coordinates(streets : nx.classes.MultiDiGraph,
    coord1: tuple, coord2 : tuple, weight = "lenght"):
    """
    Using the street network closest point to each coordinate, 
    calculate the shortest path on the networks
    coord : format (long, lat)
    return a list of osmid for the nodes of the route on the streets network
    /!\\ if there is no path return a NoneType /!\\ 
    """
    orig = ox.distance.nearest_nodes(streets,X=coord1[0],Y = coord1[1])
    dest = ox.distance.nearest_nodes(streets,X=coord2[0],Y = coord2[1])
    route = ox.shortest_path(streets, orig, dest, weight=weight) # on peut aussi mettre travel_time
    if route == None:
        print(f"/!\\ Warning /!\\ No route between {coord1} and {coord2}")

    return route

def distance_route(route, streets : nx.classes.MultiDiGraph, limit=1000):
    n = len(route)-1
    dist_metric = 0
    for i in range(n):
        adj = streets.adj[route[i]]
        dist_metric += adj[route[i+1]][0]['length']
        if dist_metric > limit:
            return dist_metric
    return dist_metric

def distance_between_coordinates(streets : nx.classes.MultiDiGraph,
    coord1: tuple, coord2 : tuple, weight = "lenght", limit = 1000):
    route = route_between_coordinates(streets,
    coord1, coord2, weight)
    if route == None:
        return np.inf
    else:
        return distance_route(route,streets)/limit

def route_between_POI(streets : nx.classes.MultiDiGraph, poi : gpd.GeoDataFrame,
    name1: str, name2 : str, weight = "lenght"):
    """
    Calculate the distance between the POI named name1 and name2
    using their centroid coordinates
    """
    coord1 = get_POI_coordinates(poi = poi, name = name1)
    coord2 = get_POI_coordinates(poi = poi, name = name2)

    route = route_between_coordinates(streets= streets, coord1= coord1,
    coord2= coord2, weight = weight)
    print(route)
    if route == None:
        print(f"/!\\ Warning /!\\ No route between {name1} and {name2}")
    return route

def get_POI_coordinates(poi: gpd.GeoDataFrame, name : str):
    coord = poi[poi['name']==name]['center'][0]
    coord = np.array(coord)
    return (coord[0], coord[1])

##########################################
##### Cuisine data exploration function ##
##########################################

def get_type_cuisine(amenities : gpd.GeoDataFrame, unique= True):
    type_restaurants = amenities['cuisine'].value_counts()
    if not unique:
        type_restaurants = {"type_cuisine":type_restaurants.keys(), "nb_cuisine":type_restaurants.values}
        df = pd.DataFrame.from_dict(type_restaurants)
        return df

    unique_keys = []
    double_keys = []
    unique_number = []
    for k in type_restaurants.keys():
        split = k.split(';')
        if k== 'chinese;japanese':
            print(split, len(split))
        if len(split) > 1:
            #sometimes the cuisine type are on the format 'italian;pizza', we will count it as 1 pizza and 1 italian
            double_keys.append(k)
        else :
            unique_keys.append(k)
    for k in double_keys: #not very optimal
        #update of the values
        for l in k.split(';'):
            if l in unique_keys:
                type_restaurants[l] += 1
            else:
                type_restaurants[l] = 1
                unique_keys.append(l)
    unique_type = {k:type_restaurants[k] for k in unique_keys}
    double_type = {k:type_restaurants[k] for k in double_keys}
    print(sum(double_type.values()), " type de cuisine en doublon ont été incorporé dans",
    sum(unique_type.values()), "style de cuisine")

    unique_type = {"type_cuisine":unique_type.keys(), "nb_cuisine":unique_type.values()}
    return pd.DataFrame(unique_type)

def counting_unique_subvalues(df : pd.DataFrame, var, relative= False):
    """
        Counting values of a categorical var in a dataframe where thoses values can have multiple subvalues.
        returns a dictionnary with for every entry the number of values counted
        the code is probably suboptimal
    """
    counts = df[var].value_counts()
    # for now we only separate list variables, could be extended to str with a split(separator)
    list_counts = counts.tolist()
    unique_values = [] #unique values are str
    multiple_values = [] #multiple are list
    for i,k in enumerate(counts.keys()):
        if type(k)==str:
            unique_values.append((i,k))
        elif type(k)==list:
            multiple_values.append((i,k))
    counts = {k:list_counts[i] for (i,k) in unique_values}
    #we transform it in dataframe because of unhashable type and we keep only the unique values, and we transform it into a dictionnary because
    for i,k in multiple_values:
        for subk in k:
            if subk in counts.keys():
                counts[subk] += list_counts[i]
            else :
                unique_values.append((i,subk))
                counts[subk] = list_counts[i]
    if relative:
        total = sum(counts.values())
        for (i,k) in counts.keys():
            counts[k]= counts[k]/total
    return counts

##########################################
##### Aggregation function ########
##########################################

def count_POI_within_polygon(gdf : gpd.GeoDataFrame,  categories = categories_tags.keys()):
    """
    count for each polygons in the polygons list how many POI correspond to each category
    take as arguments a geodataframe with polygon as index, a list of categories_tags's keys"""
    number_poi_by_cat = {}
    for cat in categories:
        number_poi_by_cat[cat] = []
        for poly, row in tqdm(gdf.iterrows()):
            n = len(get_polygon_POI_category(polygon= poly, categories=[cat]))
            number_poi_by_cat[cat].append(n)
    for cat in categories:
        gdf[cat] = number_poi_by_cat[cat] 
    return gdf

##########################################
##### 2SFCA function ########
##########################################

def calculate_2SFCA_demand(gdf,weights_by_id,
    weight_age = {
        'Ind_0_3':1,
        "Ind_4_5" : 1,
        "Ind_6_10" : 1,
        "Ind_11_17" : 1,
        "Ind_18_24" : 1,
        "Ind_25_39" : 1,
        "Ind_40_54" : 1,
        "Ind_55_64" : 1,
        "Ind_65_79" : 1,
        "Ind_80p" : 1,
        "Ind_inc" : 1
    }
):
    """
    gdf and weights_by_id must use the same id.
    """
    f = lambda row: calcule_individual_demand(row = row, weight_age= weight_age)
    demand = gdf.apply(f, axis = 'columns',raw = False)
    weighted_demand = weights_by_id.multiply(other = demand,axis=0) 
    # we have for each column i the series of individual demande j * weight ij
    return weighted_demand.sum() # donne pour chaque carré sa zone de patientèle cad le nombre de personnes
    # qui veulent recevoir le service, sommé par les poids décroissants avec la distance


def calcule_individual_demand(row,
    weight_age = {
        'Ind_0_3':1,
        "Ind_4_5" : 1,
        "Ind_6_10" : 1,
        "Ind_11_17" : 1,
        "Ind_18_24" : 1,
        "Ind_25_39" : 1,
        "Ind_40_54" : 1,
        "Ind_55_64" : 1,
        "Ind_65_79" : 1,
        "Ind_80p" : 1,
        "Ind_inc" : 1
    }):
    demand = 0
    for k in weight_age.keys():
        demand += row[k]*weight_age[k]
    return demand

def calculate_2SFCA_accessibility_var(supply,demand,weights_by_id):
    ratio = supply/demand
    weighted_ratio = weights_by_id.multiply(other = ratio,axis=0) 
    return weighted_ratio.sum()

def calculate_distanceband_weights(gdf, idCol = "IdINSPIRE",geometryCol="geometry",threshold = 1):
    # donner directement par la fonction de pysal
    # for each i in ids, we attribute the list (dataframe with  id in index) of the weight of j from i
    radius = geometry.sphere.RADIUS_EARTH_KM
    gdf.reset_index(inplace=True)
    w_db = weights.distance.DistanceBand.from_dataframe(gdf,threshold=threshold,binary = False,radius = radius,geom_col = geometryCol, ids = idCol)
    # poids calculé en faisant la fonction inverse de la distance euclidienne entre les polygons (entre leurs centroids ?)
    # thresold de 1km <=> 15mn de marche à 4km/h
    for id in gdf["IdINSPIRE"]:
        w_db[id][id]=10.0
    full_matrix,ids = w_db.full()
    max_weight = 10.0 * threshold
    weights_by_id = pd.DataFrame(index = ids)
    for i,id in enumerate(ids):
        weights_by_id = weights_by_id.join(pd.Series(index = ids,data=full_matrix[i],name=id))
        weights_by_id[id][id] = max_weight
    gdf.set_index(idCol,inplace= True)
    weights_by_id= weights_by_id/max_weight
    return weights_by_id

def calculate_2SFCA_accessibility(gdf, interestsVar, weights_by_id,weight_age={
        'Ind_0_3':1,
        "Ind_4_5" : 1,
        "Ind_6_10" : 1,
        "Ind_11_17" : 1,
        "Ind_18_24" : 1,
        "Ind_25_39" : 1,
        "Ind_40_54" : 1,
        "Ind_55_64" : 1,
        "Ind_65_79" : 1,
        "Ind_80p" : 1,
        "Ind_inc" : 1
    }
):
    """
    gdf : gpd.GeoDataFrame
    interestsVar : list of gdf's var we want to calculate the accessibility. Needs to be summable.
    Les tranches d'âge suivantes :
    Ind_0_3 : Nombre d’individus de 0 à 3 ans
    Ind_4_5 : Nombre d’individus de 4 à 5 ans
    Ind_6_10 : Nombre d’individus de 6 à 10 ans
    Ind_11_17 : Nombre d’individus de 11 à 17 ans
    Ind_18_24 : Nombre d’individus de 18 à 24 ans
    Ind_25_39 : Nombre d’individus de 25 à 39 ans
    Ind_40_54 : Nombre d’individus de 40 à 54 ans
    Ind_55_64 : Nombre d’individus de 55 à 64 ans
    Ind_65_79 : Nombre d’individus de 65 à 79 ans
    Ind_80p : Nombre d’individus de 80 ans ou plus
    Ind_inc : Nombre d’individus dont l’âge est inconnu 
    weight = decaying distance function matrix
    represented as a dataframe : col name = IdINSPIRE of the corresponding square, give a pd.Series of the weight.
    weight_age = dict with previous keys as entries, weights_by_id for each age as values
    weight_age can be for instance how relatively old people consumate health services compare to younger ones. 
    for now weight_age is unique. In the future we will implement the possibility to add one series of weights 
    for each variable.
    """
    demand = calculate_2SFCA_demand(gdf=gdf,weights_by_id=weights_by_id, weight_age=weight_age)
    # we use a unique demand function for now
    # but we can create a dict var:weight_age to take consideration that different age doesnt consume
    # the same type of services. And from that dict we create var:demand and we use that dict in
    # the following lambda function
    f = lambda s: calculate_2SFCA_accessibility_var(supply=s,demand=demand,weights_by_id=weights_by_id)
    return gdf[interestsVar].apply(f,axis = 0)